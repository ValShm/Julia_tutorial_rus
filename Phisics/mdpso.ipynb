{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "#pyplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parabol(x) = sum(u->u*u, x) \n",
    "# f(0,0) = 0, x_i ∈ [-10,10]\n",
    "\n",
    "shvefel(x) = sum(u-> -u*sin(sqrt(abs(u))), x) \n",
    "# f(420.9687,420.9687) = -819?, x_i ∈ [-500,500]\n",
    "\n",
    "rastrigin(x) = 10*length(x) + sum(u->u*u-10*cos(2*pi*u), x) \n",
    "# f(0,0) = 0, x_i ∈ [-5,5]\n",
    "\n",
    "ekly(x) = -20exp(-0.2sqrt(0.5(x[1]*x[1]+x[2]*x[2]))) - exp(0.5(cospi(2x[1])+cospi(2x[2]))) + 20 + ℯ\n",
    "# f(0,0) = 0, x_i ∈ [-5,5]\n",
    "\n",
    "rosenbrok(x) = 100(x[2]-x[1]*x[1])^2 + (x[1]-1)^2 \n",
    "# f(0,0) = 0, x_i ∈ [-5,5]\n",
    "\n",
    "bill(x) = (1.5-x[1]+x[1]*x[2])^2 + (2.25-x[1]+x[1]*x[2]*x[2])^2 + (2.625-x[1]+x[1]*x[2]^3)^2 \n",
    "# f(3,0.5) = 0, x_i ∈ [-5,5]\n",
    "\n",
    "boot(x) = (x[1]+2x[2]-7)^2 + (2x[1]+x[2]-5)^2 \n",
    "# f(1,3) = 0, x_i ∈ [-10,10]\n",
    "\n",
    "bukin6(x) = 100sqrt(abs(x[2]-0.01x[1]*x[1])) + 0.01abs(x[1]+10) \n",
    "# f(-10,1) = 0, x_i ∈ [-15,-5; -3,3]\n",
    "\n",
    "levy13(x) = sinpi(3x[1])^2 + (1+sinpi(3x[2])^2)*(x[1]-1)^2 + (1+sinpi(2x[2])^2)*(x[2]-1)^2 \n",
    "# f(1,1) = 0, x_i ∈ [-10,10]\n",
    "\n",
    "himmelblau(x) = (x[1]^2+x[2]-11)^2 + (x[1]+x[2]^2-7)^2 \n",
    "# f(3,2)... = 0, x_i ∈ [-5,5]\n",
    "\n",
    "camel3humped(x) = 2x[1]^2 - 1.05x[1]^4 + x[1]^6 /6 + x[1]*x[2] + x[2]^2 \n",
    "# f(0,0) = 0, x_i ∈ [-5,5]\n",
    "\n",
    "izom(x) = -cos(x[1])*cos(x[2])*exp(-( (x[1]-pi)^2 + (x[2]-pi)^2 )) \n",
    "# f(π,π) = -1, x_i ∈ [-100,100]\n",
    "\n",
    "holdertable(x) = -abs(sin(x[1])*cos(x[2])exp(abs( 1-sqrt(x[1]^2+x[2]^2)/pi ))) \n",
    "# f(±8.05502,±9.66459) = -19.2085, x_i ∈ [-10,10]\n",
    "\n",
    "shaffer4(x) = 0.5 + (cos(sin(abs(x[1]^2-x[2]^2)))^2-0.5) / (1+0.001(x[1]^2+x[2]^2))^2 \n",
    "# f(0,1.25313) = 0.292579, x_i ∈ [-100,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = ekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "low = [-5 -5]\n",
    "up = [5 5]\n",
    "Xs = range(low[1], stop = up[1], length = 40)\n",
    "Ys = range(low[2], stop = up[2], length = 40)\n",
    "Zs = [ fun([x y]) for x in Xs, y in Ys ];\n",
    "Zs = Zs'\n",
    "contour(Xs, Ys, Zs)\n",
    "#savefig(\"$fun surface.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum(Zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function ploter(l, u, xy, n_age, Fmin )\n",
    "    contour(Xs, Ys, Zs, fill = true);\n",
    "    scatter!(xy[1,:], xy[2,:], legend = false, xaxis=( (l[1], u[1])), yaxis=( (l[2], u[2])) );\n",
    "    title!(\"age: $n_age, f(x) = $(round(Fmin, digits = 4))\")\n",
    "    savefig(\"plots//$fun $n_age.png\")\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://habr.com/ru/post/440234/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function mdpso(;\n",
    "\tnparts = 50,\n",
    "\tndimes = 2,\n",
    "\tages = 50, # количество эпох\n",
    "\tlover = [-10 -10],\n",
    "\tupper = [10 10],\n",
    "\tC1 = [1.9 1.9],\n",
    "\tC2 = [1.8 1.8],\n",
    "\tAc = [0.1 0.1], # определяет длину шага\n",
    "\t)\n",
    "\n",
    "    minind = 0;\n",
    "    V = zeros(nparts,ndimes)\n",
    "    X = zeros(nparts,ndimes)\n",
    "    funmin = -Inf\n",
    "    Fmin = Inf\n",
    "    Fbest = fill(Fmin, nparts)\n",
    "    funx = zeros(nparts)\n",
    "    xmem = zeros(nparts,ndimes)\n",
    "    xbest = zeros(ndimes) # лучшая координата\n",
    "\n",
    "\t# частицы разбрасываются по исследуемой области\n",
    "\tfor i in 1:nparts, j in 1:ndimes\n",
    "\t\tX[i,j] = randomer(lover[j], upper[j])\n",
    "\tend\n",
    "    \n",
    "\tfor i in 1:ages\n",
    "        \n",
    "\t\tfor j in 1:nparts\n",
    "\t\t\t\n",
    "\t\t\tfunx[j] = fun(X[j,:])\n",
    "\t\t\tif funx[j] < Fbest[j]\n",
    "\t\t\t\tFbest[j] = funx[j]\n",
    "\t\t\t\txmem[j,:] = X[j,:]\n",
    "\t\t\tend\n",
    "\t\tend\n",
    "        \n",
    "        ploter(lover, upper, X, funx, i);\n",
    "        \n",
    "\t\tfunmin = minimum(funx)\n",
    "\t\tminind = argmin(funx)\n",
    "\n",
    "\t\tif funmin < Fmin\n",
    "\t\t\tFmin = funmin\n",
    "\t\t\txbest[:] = X[minind,:]\n",
    "\t\tend\n",
    "\n",
    "\t\tfor j in 1:nparts, k in 1:ndimes\n",
    "\t\t\tR1 = rand()\n",
    "\t\t\tR2 = rand()\n",
    "\t\t\tV[j,k] = Ac[k]*V[j,k] + C1[k]*R1*(xmem[j,k] - X[j,k]) + \n",
    "\t\t\t\tC2[k]*R2*(xbest[k] - X[j,k])\n",
    "\n",
    "\t\t\tX[j,k] += V[j,k]\n",
    "\t\tend\n",
    "\t\tprintln(\"Age № $i\\n xbest:\\n $(xbest[1]) $(xbest[2])\")\n",
    "\t\tprintln(\"Fmin: $Fmin\\n\")\n",
    "\tend\n",
    "    \n",
    "    f = open(\"$fun.txt\",\"w\")\n",
    "    write(f,\"C1 = $C1, C2 = $C2, Ac = $Ac, lower = $lover, upper = $upper, ages = $ages, parts = $nparts\")\n",
    "    close(f)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdpso(C1 = [0.7 0.7], C2 = [0.5 0.5], Ac = [0.09 0.09], lover = low, upper = up, ages = 10, nparts = 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Зайцев В. В. Численные методы для физиков. Нелинейные уравнения и оптимизация: учебное пособие. – Самара, 2005г. – 86с.\n",
    "* Иванов А. В. Компьютерные методы оптимизации оптических систем. Учебное пособие. –СПб: СПбГУ ИТМО, 2010 – 114с.\n",
    "* Попова Т. М. Методы многомерной оптимизации: методические указания и задания к выполнению лабораторных работ по дисциплине «Методы оптимизации» для студентов направления «Прикладная математика»/ сост. Т. М. Попова. – Хабаровск: Изд-во Тихоокеан. гос. ун-та, 2012. – 44 с.\n",
    "\n",
    "### Методы оптимизации\n",
    "\n",
    "Решение задачи оптимизации заключается в поиске значений $x_1,...,x_n$ при которых функция $а(x_1,...,x_n)$ достигает своего минимума (максимума). На практике, задачи многомерной оптимизации, заключающиеся в поиске минимумов функций нескольких переменных, встречаются гораздо чаще, нежели задачи, сводящиеся к минимизации функции одного аргумента.\n",
    "\n",
    " Задачи оптимизации часто возникают в физических исследованиях и технических разработках. Например, минимум энергии физической системы определяет ее стационарное состояние. Поиск наиболее экономичной конструкции радиоэлектронного устройства проводится минимизацией энергопотребления, выраженного в виде функции параметров системы. Иногда оптимизационные задачи появляются опосредованно как средство решения каких-либо других задач. Так, краевую задачу для системы обыкновенных дифференциальных уравнений можно решать методом пристрелки, отыскивая нулевой минимум целевой функции, сконструированный из граничных условий. \n",
    " \n",
    "На подготовитетельном этапе на тестовых функциях были опробованы ряд методов, такие как:\n",
    "* Метод покоординатного спуска, сводящий решение задачи многомерной оптимизации к многократному поиску минимума функци одного аргумента. В качестве методов одномерной оптимизиции были реализованы методы Ньютона, секущих, золотого сечения, а также методы последовательной и обратной параболической интерполяции\n",
    "* Метод наискорейшего спуска - градиентный метод, использующий для поиска минимума не только значения целевой функции, но и значения ее градиента. Поиск экстремума ведется шагами в направлении градиента (max) или антиградиента (min). На каждом шаге в направлении градиента (антиградиента) движение осуществляется до тех пор, пока функция возрастает (убывает).\n",
    "* Метод Нелдера — Мида, также известный как метод деформируемого многогранника и симплекс-метод, — метод безусловной оптимизации функции от нескольких переменных, не использующий производной(точнее — градиентов) функции, а поэтому легко применим к негладким и/или зашумлённым функциям. Суть метода заключается в последовательном перемещении и деформировании симплекса вокруг точки экстремума. Метод находит локальный экстремум и может «застрять» в одном из них. Если всё же требуется найти глобальный экстремум, можно пробовать выбирать другой начальный симплекс.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat Swarm Optimization \n",
    "\n",
    "https://www.researchgate.net/publication/221419703_Cat_Swarm_Optimization\n",
    "\n",
    "Согласно классификации биологии, семейство кошачьих содержит около тридцати двух различных видов существ, например, лев, тигр, леопард, кошка и т.д. Несмотря на то, что у них разные жизненные условия, у большинства кошачьих в поведении по-прежнему существует много общего. Для диких кошек охотничий навык обеспечивает выживание, но для домашних кошек он проявляется в виде естественного инстинкта сильного любопытства к любым движущимся вещам. Хотя они в большинстве своем проводят время неактивно.\n",
    "\n",
    "У каждой кошки есть своя собственная позиция, состоящая из $M$ измерений, скорости для каждого измерения, значение пригодности, которое представляет приспособление кошки к оптимизируемой функции, и флага, чтобы определить, находится ли кошка в режиме поиска или в режиме отслеживания. \n",
    "Окончательное решение - лучшая позиция для одной из кошек, поскольку CSO сохраняет лучшее решение до конца итераций.\n",
    "\n",
    "### Режим поиска\n",
    "\n",
    "Эта подмодель используется для моделирования ситуации с кошкой, которая отдыхает, оглядывается вокруг и ищет следующую позицию для перехода. В режиме поиска мы определяем четыре существенных фактора: пул памяти поиска (SMP), диапазон поиска для выбранного измерения (SRD), количество направлений (CDC) и самоопределение (SPC). \n",
    "\n",
    "1. SMP используется для определения размера памяти поиска для каждой кошки, который указывает точки, которые приметила кошка. Кошка выберет точку из пула памяти в соответствии с правилами, описанными ниже. \n",
    "1. SRD объявляет мутативный коэффициент для выбранных измерений. В режиме поиска, если для изменения выбран размер разницы между новым значением и старым, то агент не выйдет за пределы диапазона, который определяется SRD. \n",
    "1. CDC раскрывает, сколько измерений будет варьироваться. Все эти факторы играют важную роль в режиме поиска. \n",
    "1. SPC - это логическая переменная, которая решает, будет ли точка, в которой кошка уже стоит, одним из кандидатов для перехода. Неважно, является ли значение SPC истинным или ложным; на значение SMP это не повлияет. \n",
    "\n",
    "Как работает режим поиска, можно описать в 5 этапов следующим образом: \n",
    "\n",
    "1. Сделать $j$ копии текущей позиции $cat_k$, где j = 1:SMP. Если значение SPC равно true, пусть j = (SMP-1), а затем сохранить текущую позицию в качестве одного из кандидатов. \n",
    "1. Для каждой копии, согласно CDC, случайным образом добавляется или уменьшается SRD в процентах от текущих значений и заменяются старые.\n",
    "1. Рассчитываются значения пригодности ($f(l^k)$) всех направлений-кандидатов.\n",
    "1. Если все $f(l^k)$ не совсем равны, вычислить вероятность выбора каждой точки-кандидата методом рулетки \\cite{rulet}, в противном случае установить всю вероятность выбора каждой точки-кандидата равной 1.\n",
    "$$\n",
    "p^k=\\frac{\\displaystyle\\left|\\max_{j=1,\\dots,N}f\\left(l^j\\right)-f\\left(l^k\\right)\\right|}{\\displaystyle\\max_{j=1,\\dots,N}f\\left(l^j\\right)-\\min_{j=1,\\dots,N}f\\left(l^j\\right)}.\n",
    "$$\n",
    "1. Из пула случайно выбрать точку, к которой нужно перейти и заменить текущую позицию для $cat_k$.\n",
    "\n",
    "**Режим отслеживания** является подмоделью для моделирования случая кошки при отслеживании некоторых целей (другой кошки с лучшими координатами).\n",
    "Как только кошка переходит в режим отслеживания, она движется в соответствии со своими собственными скоростями для каждого измерения. Действие режима трассировки можно описать в 3 этапа следующим образом:\n",
    "\n",
    "1. Обновить скорости для каждого измерения ($v_{k, d}$) в соответствии с \n",
    "$$\n",
    "v_{k,d} = v_{k,d} + r_1\\times c_1 \\times (x_{best,d} - x_{k,d}), \\ d=1,2,...,M\n",
    "$$\n",
    "\n",
    "1. Проверить, находятся ли скорости в диапазоне максимальной скорости. Если новая скорость выходит за пределы допустимого диапазона, установить ее равной пределу.\n",
    "1. Обновить положение $cat_k$ в соответствии с уравнением \n",
    "$$\n",
    "x_{k,d} = x_{k,d} + v_{k,d}\n",
    "$$\n",
    "\n",
    "Как мы описали в вышеприведенном подразделе, CSO включает две подмодели: режим поиска и режим трассировки. Чтобы объединить два режима в алгоритм, мы определяем коэффициент смешивания (TS) режима поиска с режимом трассировки.\n",
    "\n",
    "Поведение бега за целями кошки применяется в режиме трассировки. Следовательно, TS должно иметь достаточно большое значение, чтобы гарантировать, что кошки проводят большую часть времени в режиме поиска, как в реальном мире. \n",
    "Процесс CSO можно описать в 6 этапов следующим образом:\n",
    "\n",
    "1. Создать N кошек. \n",
    "1. Случайным образом разогнать кошек в пространстве M-мерного решения и случайным образом выбрать значения, которые находятся в пределах максимальной скорости, для скоростей каждой кошки. Затем случайно выбрать количество кошек и перевести их в режим отслеживания согласно TS, а остальные переведите в режим поиска. \n",
    "1. Оценить пригодность каждой кошки, подставляя положения кошек в функцию пригодности, которая представляет критерии нашей цели, и сохранить координаты лучшей кошки в памяти. \n",
    "1. Переместить кошек в соответствии с их флагами. Если $cat_k$ находится в режиме поиска, инициировать алгоритм поиска, в противном случае применить процесс отслеживания. Этапы процесса представлены выше. \n",
    "1. Снова выбрать количество кошек и установить их в режим отслеживания в соответствии с TS, затем перевести других кошек в режим поиска. \n",
    "1. Проверить условие завершения, если оно выполнено, прекратить работу программы. В противном случае повторить шаги с 3 по 5.\n",
    "\n",
    "Нетрудно понять, что метод оптимизации стаей кошек предоставляет более интеллектуальный и интерактивный поиск глобального минимума по сравнению с методом роя частиц. Лучшей стратегией поиска является исследование функционала и поиск глобального минимума с помощью роевого метода с последующим уточнением параметров с помощью одного из методов многомерной оптимизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://habr.com/ru/post/328760/\n",
    "\n",
    "Псевдокод алгоритма представлен ниже:\n",
    "\n",
    "Шаг 1. Инициализировать популяцию из N котов.\n",
    "\n",
    "Шаг 2. Определить приспособленность каждого кота на основе его позиции в исследуемом пространстве. Запомнить \"лучшего\" кота (в терминологии задачи минимизации, ему будет соответствовать наименьшее значение функции).\n",
    "\n",
    "Шаг 3. Переместить котов в соответствии с их процедурой смещения (поиск или погоня).\n",
    "\n",
    "Шаг 4. Заново присвоить котам режимы перемещения в соответствии с параметром MR.\n",
    "\n",
    "Шаг 5. Проверить условие окончания работы. В случае его невыполнения перейти к шагу 2.\n",
    "\n",
    "* количество котов (numberOfCats) — чем больше котов, тем дольше время работы алгоритма (если его ограничивать количеством итераций), но и потенциально большая точность найденного ответа,\n",
    "* пропорция котов в режиме поиска и погони (MR) — данный параметр позволяет направлять поиск по той стратегии, которую пользователь считает предпочтительной; например, если вы заведомо знаете окрестность, в которой лежит глобальный оптимум, то логично инициализировать популяцию в этой окрестности и поддерживать большее число котов-исследователей в популяции для уточнения первоначального решения,\n",
    "* количество попыток для режима поиска (SMP) — сколько разных смещений будет производить кот-исследователь; большие значения данного параметра увеличивают время одной итерации, но позволяют увеличить точность определения положения экстремума,\n",
    "* доля смещения для режима поиска (SRD) — доля, на которую кот-исследователь смещается относительно своего текущего положения, большие значений смещают уточняющий поиск в сторону глобального,\n",
    "* количество направлений, по которым ведется поиск (CDC) — данный параметр регулирует количество измерений, которые будут изменять у текущего положения кота, находящегося в режиме поиска; меньшие значения делают поиск покоординатным,\n",
    "* желание остаться на старом месте (SPC) — булева переменная, которая позволяет выбирать, может ли кот-исследователь остаться не текущем месте,\n",
    "* константа скорости (velocityConstant) — определяет степень поворотливости кота во время погони; большие значения быстрее меняют текущий вектор скорости кота,\n",
    "* максимальная скорость (velocityRatio) — все-таки вы в доме хозяин, поэтому в случае если кто-то из котов слишком уж разогнался, то вы вполне можете на него прикрикнуть,чтобы он притормозил, т.о. данный параметр ограничивает максимальную скорость движения котов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cso(fun;\n",
    "    ncats = 20,\n",
    "    ndimes = 2,\n",
    "    ages = 20, # количество эпох\n",
    "    TS = 0.5, # Seeking / Tracing\n",
    "    SMP = 5, # Seek Memory Pool\n",
    "    lover = [-10.; -10.],\n",
    "    upper = [10.; 10.],\n",
    "    dx = [ 0.1, 0.1], \n",
    "    a = 0.1 ) # acceleration\n",
    "\n",
    "    randomer(a::Float64,b::Float64) = rand()*(b-a)+a\n",
    "    minind = 0\n",
    "    Fmin = Inf\n",
    "    #xbest = zeros(ndimes) # лучшая координата\n",
    "    P = zeros(SMP) # probabilities\n",
    "    funx = zeros(SMP)\n",
    "    dirs = zeros( ndimes, SMP )\n",
    "    #Fbest = fill(Fmin, ncats)\n",
    "    V = zeros(ndimes,ncats)\n",
    "    X = zeros(ndimes,ncats)\n",
    "    \n",
    "    for i in 1:ndimes, j in 1:ncats\n",
    "        X[i,j] = randomer(lover[i], upper[i])\n",
    "    end\n",
    "    \n",
    "    Fbest = [ fun(X[:,j]) for j = 1:ncats ]\n",
    "\n",
    "    Fmin, minind = findmin(Fbest)\n",
    "    xbest = X[:,minind]\n",
    "\n",
    "\n",
    "    for i in 1:ages\n",
    "        \n",
    "        for j in 1:ncats\n",
    "            \n",
    "            if rand() < TS #seeking\n",
    "                for k in 1:SMP\n",
    "                    dirs[:,k] = X[:,j] + rand()*dx\n",
    "                    funx[k] = fun(dirs[:,k])\n",
    "                end\n",
    "                #\n",
    "                minf, maxf = extrema( funx )\n",
    "                \n",
    "                P = [ abs(maxf - funx[k]) / (maxf - minf) for k = 1:SMP ]\n",
    "                \n",
    "                Ps = cumsum( [0.;P] )\n",
    "                r = rand()\n",
    "                loci = 1\n",
    "                for k = 1:SMP\n",
    "                    if Ps[k] < r < Ps[k+1]\n",
    "                        loci = k\n",
    "                        break\n",
    "                    end\n",
    "                end\n",
    "                #\n",
    "                #loci = argmin(funx) # так лучше?\n",
    "                \n",
    "                X[:,j] .= dirs[:,loci]\n",
    "                if funx[loci] < Fbest[j]\n",
    "                    Fbest[j] = funx[loci]\n",
    "                end\n",
    "                \n",
    "                \n",
    "            else # tracing\n",
    "                V[:,j] .+= rand()*a*(xbest - X[:,j])\n",
    "                X[:,j] .+= V[:,j]\n",
    "                \n",
    "                Ftr = fun( X[:,j] )\n",
    "                if Ftr < Fbest[j]\n",
    "                    Fbest[j] = Ftr\n",
    "                end\n",
    "                \n",
    "            end\n",
    "        end\n",
    "        Fbox, minind = findmin(Fbest)\n",
    "        if Fbox < Fmin\n",
    "            Fmin = Fbox\n",
    "            xbest .= X[:,minind]\n",
    "        end \n",
    "    ploter(lover, upper, X, i, Fmin);\n",
    "        println(\"Age № $i\\n xbest:\\n $xbest\")\n",
    "        println(\"Fmin: $Fmin\\n\")\n",
    "    end\n",
    "    \n",
    "    #f = open(\"$fun.txt\",\"w\")\n",
    "    #write(f,\"C1 = $C1, C2 = $C2, Ac = $Ac, lower = $lover, upper = $upper, ages = $ages, parts = $nparts\")\n",
    "    #close(f)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cso(ekly, ages = 20, lover = [-5., -5.], upper = [5., 5.], dx = [0.1, 0.1], a = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
